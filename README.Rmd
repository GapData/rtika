---
output: github_document
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# rtika
***Extract text or metadata from over a thousand file types.***

[![Travis-CI Build Status](https://travis-ci.org/predict-r/rtika.svg?branch=master)](https://travis-ci.org/predict-r/rtika)
[![Coverage status](https://codecov.io/gh/predict-r/rtika/branch/master/graph/badge.svg)](https://codecov.io/github/predict-r/rtika?branch=master)


>Apache Tika is a content detection and analysis framework, written in Java, stewarded at the Apache Software Foundation. It detects and extracts metadata and text from over a thousand different file types, and as well as providing a Java library, has server and command-line editions suitable for use from other programming languages ...

>For most of the more common and popular formats, Tika then provides content extraction, metadata extraction and language identification capabilities. (From <https://en.wikipedia.org/wiki/Apache_Tika>, accessed Jan 18, 2018)

This R interface includes the Tika software.

## Installation
You only need R and either `OpenJDK 1.7` or `Java 7`. Higher versions work. To check your version, run the command `java -version` from a terminal. Get Java installation tips at <http://openjdk.java.net/install/> or <https://www.java.com/en/download/help/download_options.xml>. 

On Windows, the `curl` package is suggested if you feed `rtika` with urls instead of local documents. 

Next, install the `rtika` package from github.com.

```{r,results = "hide",message = FALSE }
# Okay, we also need devtools to easily install from github, until this package is on CRAN 
if(!base::requireNamespace('devtools')){base::install.packages('devtools',repos='https://cloud.r-project.org')};
# Install and setup
if(!base::requireNamespace('rtika')){devtools::install_github('predict-r/rtika')};
library('rtika')  
```

There are no dependencies other than `java`. It's nice that `rtika` is enhanced by other packages.
```{r,results = "hide",message = FALSE }
# The curl, sys, and data.table packages enhance rtika. Magrittr helps document long pipelines.
library("magrittr")
```

## Extract Plain Text
Describe the paths to files that can contain text, such as `.pdf`, Microsoft Office (`.doc`, `docx`, `.ppt`, etc.), `.rtf`, or a mix. Tika reads each file, identifies the format, invokes a specialized parser, and then `tika_text()` returns a plain text rendition for you. 

**The `rtika` package processes batches of documents efficiently**, so I recommend batches. Currently, all the `rtika` functions take a tiny bit of time to spin up, and that will get annoying with hundreds of separate calls to `tika_text()` and the others. 
```{r}
# Files or urls
batch <- c('https://cran.r-project.org/doc/manuals/r-release/R-data.pdf','https://cran.r-project.org/doc/manuals/r-release/R-lang.html')

# A short data pipleine, shown with magrittr:
text <- {
  batch %>%
  tika_text() 
}

# Normal syntax works, e.g. text = tika(input)

# Look at the structure. It's a character vector:
utils::str(text)

```

The batch is two urls, and the output `text` is a character vector of two documents. See a snippet of the first document using `base::cat()`.
```{r}
# Look at a snippet from the long first document:
base::cat(base::substr(text[1],45,160)) 
```

 
 Now we have some plain text. If there was a problem, the result would be `as.character(NA)`. Plain text can be easy to tokenize. 
```{r}
tokenize_words <- function(x){w=base::strsplit(base::tolower(x),split='[^a-zA-Z]+');base::lapply(w,function(x)x[x!=''])}

# Make a List of documents, each with a word vector
words <- {
  text %>% 
  tokenize_words()
}

# Look at the structure
str(words)
```
Access the first document in the `List` with the `[[` syntax. Each contains words:
```{r}
words[[1]][1:7] 
words[[2]][1:7] 
```
While `rtika` is efficient for batches, to further reduce the time for very large jobs, increase the number of parallel system threads. For example: `tika_text(threads=2)`. This works on all `rtika` functions.
```{r}
text <- {
  batch %>%
  tika_text(threads=2) 
}
```

## Get Metadata
Metadata comes with the `jsonRecursive`,`xml` and `html` output options. In addition, text will be in `XHTML`, the stricter `HTML`. That retains more information than plain text, and is nice for extracting tables and table cells. The special `jsonRecursive` mode can also process compressed archives of documents. It is quickly accessed with `tika_json()`.
```{r}
# Input vector of length two:
batch <- c('https://cran.r-project.org/doc/manuals/r-release/R-data.pdf','https://cran.r-project.org/doc/manuals/r-release/R-lang.html')

# With tika_json(), text will be XHTML in the `X-TIKA:content` field.
metadata <- {
  batch %>%
  tika_json() %>% # output is a character with as.character(NA) failures
  base::ifelse(is.na(.),'[{"X-TIKA:content":""}]',.)  %>% # typical failures handled
  base::lapply(jsonlite::fromJSON) # list of data.frames
}

```
See the structure of the metadata, or meta-metadata.

```{r}
# The first document's metadata:
utils::str(metadata[[1]])
```


## Similar Packages

There is some overlap with many other text parsers, such as the R interface to antiword (See: <https://github.com/ropensci/antiword>). Listing all of them would take a huge amount of space, since Apache Tika processes over a thousand file types (See: <https://tika.apache.org/>). The main difference is that instead of specializing on a single format, Tika integrates dozens of specialist libraries from the Apache Foundation. Tika's unified approach offers a bit less control, and in return eases the parsing of digital archives filled with possibly unpredictable file types.

In September 2017, github.com user *kyusque* released `tikaR`, which uses the `rJava` package to interact with Tika (See: <https://github.com/kyusque/tikaR>). As of writing, it provided similar text and metadata extraction, but only `xml` output. 

Back in March 2012, I started a similar project to interface with Apache Tika. My code also used low-level functions from the `rJava` package. I halted development after discovering that the Tika command line interface (CLI) was easier to use. My empty repository is at <https://r-forge.r-project.org/projects/r-tika/>.

 I chose to finally develop this package after getting excited by Tika's new 'batch processor' module, written in Java. I found the batch processor has very good efficiency when processing tens of thousands of documents. Further, it is not too slow for a single document either, and handles errors gracefully. Connecting `R` to the Tika batch processor turned out to be relatively simple, because the `R` code is simple. It uses the CLI to point Tika to the files. Simplicity, along with continuous testing, should ease integration. I anticipate that some researchers will need plain text output, while others will want `json` output. Some will want multiple processing threads to speed things up. These features are now implemented in `rtika`, although apparently not in `tikaR` yet. 