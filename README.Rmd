---
output: github_document
---
# rtika
***Extract text or metadata from over a thousand file types.***

[![Travis-CI Build Status](https://travis-ci.org/predict-r/rtika.svg?branch=master)](https://travis-ci.org/predict-r/rtika)
[![Coverage status](https://codecov.io/gh/predict-r/rtika/branch/master/graph/badge.svg)](https://codecov.io/github/predict-r/rtika?branch=master)


>Apache Tika is a content detection and analysis framework, written in Java, stewarded at the Apache Software Foundation. It detects and extracts metadata and text from over a thousand different file types, and as well as providing a Java library, has server and command-line editions suitable for use from other programming languages ...

>For most of the more common and popular formats, Tika then provides content extraction, metadata extraction and language identification capabilities. (Accessed Jan 18, 2018. See <https://en.wikipedia.org/wiki/Apache_Tika>.)

This R interface includes the Tika software.

## Installation
You only need `Java 7` or `OpenJDK 1.7`. Higher versions work. To check your version, run the command `java -version` from a terminal. Get Java installation tips at <http://openjdk.java.net/install/> or <https://www.java.com/en/download/help/download_options.xml>. 

On Windows, the `curl` package is suggested if the documents are described by urls.

Next, install the `rtika` package from github.com. `rtika` has no other dependencies.

```{r,results = "hide",message = FALSE }
# install
if(!requireNamespace('devtools')){install.packages('devtools', repos='https://cloud.r-project.org') }
if(!requireNamespace('rtika')){devtools::install_github('predict-r/rtika') } 
library('rtika') 

# There are no other dependencies, but curl, sys, data.table are suggested.
# magrittr is suggested for it's pipe function %>%
library("magrittr")
```



## Extract Plain Text
Describe the paths to files that contain text, such as PDF, Microsoft Office (`.doc`, `docx`, `.ppt`, etc.), `.rtf`, or a mix. Tika reads each file, identifies the format, invokes a specialized parser, and returns a plain text rendition.
```{r,results = "hide",warning = FALSE}
#files or urls to text
text = {
  'https://cran.r-project.org/doc/manuals/r-release/R-data.pdf' %>%
  tika_text() 
}
# also works:
# text = tika('https://cran.r-project.org/doc/manuals/r-release/R-data.pdf') 
```

In this case, the input is a single url, and so the `text` is of length 1. Display a snippet using `cat`.
```{r,comment=NA }
cat(substr(text[1],45,160)) # sub-string of the text
```
If instead of a single file, a batch of files or urls was sent to `tika`, the text would be in a longer vector with the same order and length as the input. In fact, Tika processes batches of documents efficiently, and I recommend batches. Tika takes a tiny bit of time to spin up each time, and that will get annoying with hundreds of separate calls.  
 
 Now that we have plain text, getting the words is relatively easy:
```{r,comment=NA}
tokenize_words = function(txt){w =strsplit(tolower(txt[1]),split='[^a-zA-Z]+')[[1]]; w[w!='']}

words = {
  text %>% 
  tokenize_words() 
}

words[1:7] 
```
## Get Metadata
Metadata comes with the `jsonRecursive`,`xml` and `html` output options. The text with these will be HTML by default and retain more formatting, such as table cells.  With `jsonRecursive`, the text will be in the `X-TIKA:content` field.
```{r,results = "hide",warning = FALSE,message = FALSE}
# 'tika_json()' is a shortcut for 'jsonRecursive' mode'

metadata = {
  c('https://cran.r-project.org/doc/manuals/r-release/R-data.pdf','https://cran.r-project.org/doc/manuals/r-release/R-lang.html') %>%
  tika_json() %>%
  sapply(jsonlite::fromJSON)
}

```
See the structure of the metadata, or meta-metadata.

```{r,comment=NA}
str(metadata[[1]]) #list of data.frames, one for each document
```

## Similar Packages

There is some overlap with many other text parsers, such as the R interface to antiword (See: <https://github.com/ropensci/antiword>). Listing all of them would take a huge amount of space, since Apache Tika processes over a thousand file types (See: <https://tika.apache.org/>). The main difference is that instead of specializing on a single format, Tika integrates dozens of specialist libraries from the Apache Foundation. Tika's unified approach offers a bit less control, and in return eases the parsing of digital archives filled with possibly unpredictable file types.

In September 2017, github.com user *kyusque* released `tikaR`, which uses the `rJava` package to interact with Tika (See: <https://github.com/kyusque/tikaR>). As of writing, it provided similar text and metadata extraction, but only `xml` output. 

Back in March 2012, I started a similar project to interface with Apache Tika. My code also used low-level functions from the `rJava` package. I halted development after discovering that the Tika command line interface (CLI) was easier to use. My empty repository is at <https://r-forge.r-project.org/projects/r-tika/>.

 I chose to finally develop this package after getting excited by Tika's new 'batch processor' module, written in Java. I found the batch processor has very good efficiency when processing tens of thousands of documents. Further, it is not too slow for a single document either, and handles errors gracefully. Connecting `R` to the Tika batch processor turned out to be relatively simple, because the `R` code is simple. It uses the CLI to point Tika to the files. Simplicity, along with continuous testing, should ease integration. I anticipate that some researchers will need plain text output, while others will want `json` output. Some will want multiple processing threads to speed things up. These features are now implemented in `rtika`, although apparently not in `tikaR` yet. 