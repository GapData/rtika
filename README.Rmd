---
output: github_document
---
# rtika
***Extract text or metadata from over a thousand file types.***

[![Travis-CI Build Status](https://travis-ci.org/predict-r/rtika.svg?branch=master)](https://travis-ci.org/predict-r/rtika)
[![Coverage status](https://codecov.io/gh/predict-r/rtika/branch/master/graph/badge.svg)](https://codecov.io/github/predict-r/rtika?branch=master)


>Apache Tika is a content detection and analysis framework, written in Java, stewarded at the Apache Software Foundation. It detects and extracts metadata and text from over a thousand different file types, and as well as providing a Java library, has server and command-line editions suitable for use from other programming languages ...

>For most of the more common and popular formats, Tika then provides content extraction, metadata extraction and language identification capabilities. (Accessed Jan 18, 2018. See <https://en.wikipedia.org/wiki/Apache_Tika>.)

This R interface includes the Tika software.

## Installation
You only need `Java 7` or `OpenJDK 1.7`. Higher versions work. To check your version, run the command `java -version` from a terminal. Get Java installation tips at <http://openjdk.java.net/install/> or <https://www.java.com/en/download/help/download_options.xml>. 

On Windows, the `curl` package is suggested if there are documents to process on a remote server.

Next, install the `rtika` package from github.com. It has no other dependencies.

```{r,results = "hide",message = FALSE }
# devtools simplifies installation from Github.
if(!requireNamespace('devtools')){ install.packages('devtools', repos='https://cloud.r-project.org') }
# Install rtika from github
if(!requireNamespace('rtika')){ devtools::install_github('predict-r/rtika') } 
library('rtika') 
# There are no other dependencies, but curl, sys and magrittr are suggested.
library("magrittr")
```



## Extract Plain Text
Describe the paths to files that contain text, such as `.pdf`, `.doc`, `.docx`, `.rtf`, `.ppt`, or a mix. Tika reads each selected file, identifies the format, parses the `.pdf` in this case, and return a plain text rendition.
```{r,results = "hide",warning = FALSE}
files_or_urls = 'https://cran.r-project.org/doc/manuals/r-release/R-data.pdf'
text = files_or_urls %>%  tika()
# text = tika(files_or_urls) # also works
```

The `text` will be a UTF-8 character vector, in the same order as the `input`. Display a snippet using `cat`.  
```{r,comment=NA }
cat(substr(text[1],45,450)) # sub-string of the text

```
Get the words:
```{r,comment=NA}
tokenize_words <- function(txt){w =strsplit(tolower(txt[1]),split='[^a-zA-Z]+')[[1]]; w[w!='']}
words = text %>% tokenize_words()
words[1:7] 
```
## Get Metadata
Metadata comes with the `json`,`xml` and `html` output options. A side effect is that Tika retains more document structure, such as table cells.
```{r,results = "hide",warning = FALSE,message = FALSE}
# 'J' is a shortcut for 'jsonRecursive'
metadata = files_or_urls %>% tika('J') %>% jsonlite::fromJSON()
```
See the structure of the metadata, or meta-metadata ðŸ¤¯ .

```{r,comment=NA}
str(metadata) #data.frame of metadata
```
```{r,echo=FALSE}
# unlink(dinput_ir,recursive = TRUE)
```
## Similar Packages

There is some overlap with many other text parsers, such as the R interface to antiword (See: <https://github.com/ropensci/antiword>). Listing all of them would take a huge amount of space, since Apache Tika processes over a thousand file types (See: <https://tika.apache.org/>). The main difference is that instead of specializing on a single format, Tika integrates dozens of specialist libraries from the Apache Foundation. Tika's unified approach offers a bit less control, and in return eases the parsing of digital archives filled with possibly unpredictable file types.

In September 2017, github.com user *kyusque* released `tikaR`, which uses the `rJava` package to interact with Tika (See: <https://github.com/kyusque/tikaR>). As of writing, it provided similar text and metadata extraction, but only `xml` output. 

Back in March 2012, I started a similar project to interface with Apache Tika. My code also used low-level functions from the `rJava` package. I halted development after discovering that the Tika command line interface (CLI) was easier to use. My empty repository is at <https://r-forge.r-project.org/projects/r-tika/>.

 I chose to finally develop this package after getting excited by Tika's new 'batch processor' module, written in Java. I found the batch processor has very good efficiency when processing tens of thousands of documents. Further, it is not too slow for a single document either, and handles errors gracefully. Connecting `R` to the Tika batch processor turned out to be relatively simple, because the `R` code is simple. It uses the CLI to point Tika to the files. Simplicity, along with continuous testing, should ease integration. I anticipate that some researchers will need plain text output, while others will want `json` output. Some will want multiple processing threads to speed things up. These features are now implemented in `rtika`, although apparently not in `tikaR` yet. 